	\chapter{Methodology}
	\label{ch:method}
This chapter details the methods used in the analysis of Nêhiyawêwin Order. The primary research question investigated in this dissertation is: how, and in what way, can Nêhiyawêwin order be understood as an alternation that can be predicted through morphosyntactic, surface-syntactic, and lexical-semantic features. This chapter describes the corpus used, the univariate analysis, and the multivariate analysis. The methodologies used in this analysis are based off those bivariate and multivariate statistics described in  \citet{divjak2006ways, bresnan2007predicting, gries2003multifactorial} and \citet{arppe2008univariate}, in particular the combination of univariate and multivariate techniques. This chapter does not detail the methods used for creating the underlying corpus (information detailed at length in \cite{arppe1945morphosyntactically}) or the process by which verbs and nouns were semantically clustered for inclusion as predictors (described in Chapter \ref{ch:semantics}).

\section{The Corpus}
The underlying corpus from which the data set used in this dissertation is the Ahenakew-Wolfart corpus \citep{arppe1945morphosyntactically}. The Ahenakew-Wolfart corpus is likely the largest morphosyntactically tagged corpus of all Canadian Indigenous languages, let alone Nêhiyawêwin. Although there has been attemtps in the last few decades to increase the amount of texts in Nêhiyawêwin, there is still a paucal amount of texts written in Nêhiyawêwin, and many of those texts that are publicly available, are written in a nonstandard Roman orthography. The Ahenakew-Wolfart corpus is unique in that it is meticulously standardized. The texts that make up the corpus were collected by Freda Ahenakew and H. C. Wolfart between the 1970s and 1990s. These texts have previously been published in \citet{AhenakewAlice2000, Bearetal1992, KaNipitehtew1998, Masuskapoe2010piko, Minde1997kwayask, VandallDouquette1987,Whitecalf1993}. These texts are mainly dialectic or narrative discussions between two or more native Nêhiyawêwin speakers. Together, these texts contain 142,192 tokens (20,503 types), though some of these tokens are English, French, or Michif words; fragments; or other items. Focusing only on Nêhiyawêwin items, there are 80,221 tokens (16,532 types). Each of these tokens has been morphosyntactically tagged by automatic and hand-parsed means \citep{arppe1945morphosyntactically}. Tokens were tagged for their lemma as well as both verbal and nominal features. For verbs: preverbs, tense, word class, Order, commitative morphemes, and conjugation class; For nouns: person/number marking, possession, declension, and diminutive morphemes; Both nouns and verbs were marked for the feature of semantic class. An example token with its relevant tags is found in (\ref{tag}).

\begin{exe}
\ex
\gll ê-ohci-pimâtisit \\
\codebox{pimâtisiw PV/e PV/ohci V AI Cnj 3Sg @PRED-AI} \\
\trans `S/he lived thus / make a living thus'
\label{tag}
\end{exe}

Beyond this, the corpus has been further syntactically tagged by an automatic constraint grammar (Schmirler et al. 2018, Schmirler Forthcoming). Among other features, this constraint grammar marks tokens for their predicate, actor, and goal status. 

To create the data set used in this dissertation, I extracted only verbs from the above corpora and further restricted the data set by selecting only verbs that contained a classification as described in Chapter \ref{ch:semantics}. This results in a data set of 13,628 tokens (2032 types). In addition to the morphosyntactic tags seen above, verbs were marked for arguments (and those arguments' morphosyntactic features) when arguments were syntactically present (as opposed to represented only by verbal agreement). This results in an entry such as (\ref{tagplus}). 

\begin{exe}
\ex
\codebox{pimâtisiw PV/e PV/ohci V AI Cnj 3Sg @PRED-AI AI-state \\ kikâwînaw N A D Px1Sg Sg @ACTOR> NDA-Relations } \\
\label{tagplus}
\end{exe}

From here, each token and its accompanying analyses were transformed into a data frame of `dummy' variables: every verb lemma token makes up a row, while every morphosyntactic tag constitutes a logical column. For every lemma token, if a morphosyntactic feature is observed, a value of \codebox{TRUE} is set for the corresponding column, otherwise a value of \codebox{FALSE} is set. Dummy variables allow for easily interpreted results, especially when dealing with covariance \citep{baayen2012mixed}. Given the example of (\ref{tagplus}) the data frame extract in Table \ref{dfex} is produced.



\begin{table}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{llllllllllllll} \\
\toprule
Lemma     & PRED-AI          & PV/ahci          & ...          & PV/e           & PV/ohci       & PV/pe          & V             & AI            & Cnj           & 3.actor           & 3.goal & AI-state & Sg.actor \\
\midrule
pimâtisiw & \codebox{TRUE}    & \codebox{FALSE}   & \codebox{...} & \codebox{TRUE}  & \codebox{TRUE} & \codebox{FALSE} & \codebox{TRUE} & \codebox{TRUE} & \codebox{TRUE} & \codebox{TRUE} & \codebox{FALSE} & \codebox{TRUE} & \codebox{TRUE}
 \\
\bottomrule
\end{tabular}}
\caption{Extract from Data Frame \label{dfex}
}
\end{table}

For the sake of fitting Table \ref{dfex} to the page, the majority of the columns are not shown, but every feature present in (\ref{tagplus}) would have a column value of \codebox{TRUE} for the token \codebox{pimâtisiw}, and all features not present are given a value of \codebox{FALSE}. The exception to this is the actor and goal marking morphemes. Although the corpus marks person and number of morpheme as one unit (e.g. \codebox{1Sg}), the data frame used for analysis in this dissertation split the features up (i.e. there were seperate columns for \codebox{3} and \codebox{Sg} for both actors and goals). Finally, a number of tokens were removed because their part of speech was not reliably identified in the corpus. There were 310 of these tokens, the majority of which (301 tokens) were the verb \textit{ayâw}. In addition to basic locative use, \textit{ayâw} may also be used to describe the state of `having' something. In the corpus, \textit{ayâw} was marked as both \codebox{VAI} and \codebox{VTI}. Because the VTI form of the verb inflects the same as the VAI form, and because syntactic arguments are usually not present in a sentence beyond verbal agreement (and even then, only in the VTA), determining which of these two classes the lemma was acting in was difficult for the non-native speakers annotating the corpus. Three further lemmas, \textit{manitowi-kîsikâw} (4 tokens), \textit{misi-paskwâw} (3 tokens), and \textit{nanamipayiw} (1 token), were removed as there was disagreement between the corpus and dictionary sources. In the first two cases, these forms were given in the corpus as VIIs, while dictionary sources cited them as also NIs. This disagreement is understandable, as VIIs that deal with time or space often describe substantives. The final case, \textit{nanamapayiw} is given as an VII in the corpus, while \citet{Wolvengrey2001} analyses it as an VAI and \citet{leclaire1998alberta} offers an analysis of both VAI and VII. Although context (through either native speaker annotation or translations by native speakers) would quickly resolve these ambiguities, the corpus being used had not yet been disambiguated in this sense, and given the small number of tokens, I opted to remove these 309 tokens from the data set. 

Because Nêhiyawêwin contains a large number of possible preverbs (the model underlying the corpus could identify 267 unique preverbs), I undertook a manual classification of these morphemes. I identified 8 unique classes: \codebox{Discourse}, \codebox{Position}, \codebox{Qual}, \codebox{Quant}, \codebox{Time}, \codebox{Move}, \codebox{Start/Finish} and \codebox{Want/Can}. Of the 267 identified preverbs, only 86 preverb types were observed in the corpus. Table \ref{tab:pv} lists the number of tokens and types in each of the preverb classes.

\begin{table}
\centering
\begin{tabular}{lrr} \\
\toprule
             & Types & Tokens \\
\midrule

Discourse    & 4     & 277    \\
Position     & 15    & 285    \\
Qual         & 30    & 316    \\
Quant        & 7     & 10     \\
Time         & 18    & 4720   \\
Move         & 4     & 731    \\
Start/Finish & 5     & 229    \\
Want/Can     & 4     & 195   \\
\bottomrule
\end{tabular}
\caption{Preverb Class Tokens and Types \label{tab:pv}
}
\end{table}

In all, the resulting data frame of non-imperative forms contains 13,292 lemma rows by 4777 columns. Due to errors in coding, 100 items were excluded from this, creating a data frame of 13,192 items. The use of such a logical data frame for predicting an alternation is present in \citet{arppe2008univariate} and allows for the assessment of individual values of categorical variables through straightforward application of chi-squared analyses and logistic regression to predict a multinomial alternation, in this case Order. 

\section{Modelling the Alternation}
In this dissertation, I will evaluate a univariate analysis given the morphosemantic features mentioned above to model a verb lemma's likelihood of occurring in various Order types. Although Chapter \ref{ch:order} identified five unique Conjunct Orders (along with the Independent), the majority of these classes have few tokens. Small counts can be problematic for statistical analyses, particularly for regression analyses. To address this, the ka-/ta-Initial, Initial Change, and Subjunctive Conjuncts were conglomerated into  a single `other' class. This results in the Order alternations as seen in Table \ref{tab:corporder}.

\begin{table}
\centering
\begin{tabular}{llrr} \\
\toprule
            &   & Types & Tokens \\
\midrule
Independent    &                      &  876  & 4390   \\
ê-Conjunct     &                      & 1480  & 6378   \\
kâ-Conjunct    &                      &  600  & 1696   \\
Other-Conjunct &                      &  393    & 828   \\
               & Subjunctive          &  75   & 100      \\
               & Initial Change       &  18   & 21      \\
               & ka-Conjunct          &  344 & 707      \\
               
\midrule
Total & &  3349  & 13,294   \\
\bottomrule
\end{tabular}
\caption{Preverb Class Tokens and Types \label{tab:corporder}
}
\end{table}

In order to gain a wholistic understanding of Nêhiyawêwin Order, this dissertation will investigate 3 main alternations of these Orders:

\begin{itemize}
    \item Independent vs Conjunct
    \item Independent vs ê-Conjunct
    \item Conjunct Type: ê-Conjunct vs kâ-Conjunct vs Other-Conjunct
\end{itemize}

The first of these alternations, Independent vs Conjunct, will inform about the difference between the two Orders broadly. The second, Independent vs ê-Conjunct, will investigate the difference between the two most similar Order forms which are often conceived as synonyms and used roughly interchangeably. The third alternation will be used to model the extent to which we can predict the modes through morphosemantic features from a corpus. Three main data frames were used:

\begin{itemize}
    \item \codebox{AWnImp}: used in analyzing the Imperative vs. Conjunct alternation, representing all non-imperative forms minus the 100 errors previously mentioned
    \item \codebox{AWIvE}: used in analyzing the Independent vs. ê-Conjunct alternation, representing only forms of forms with \codebox{TRUE} \codebox{Ind} or \codebox{PV.e} forms
    \item \codebox{AWCnj}: used in analyzing the Conjunct Type alternation, representing only forms with \codebox{TRUE} \codebox{Cnj} forms. 
\end{itemize}

In Table (\ref{tab:corpstats}) through (\ref{tab:corpstats3}) are relevant counts for each of the three dataframes.

\begin{table}
\centering
\begin{tabular}{lrr} \\
\toprule
               & Types & Tokens \\
\midrule
Independent    &  876  & 4390   \\
Conjunct     & 1722  & 8802   \\
\midrule
Total &  2598  & 13,192   \\
\bottomrule
\end{tabular}
\caption{\codebox{AWnImp} statistics \label{tab:corpstats}
}
\end{table}


\begin{table}
\centering
\begin{tabular}{lrr} \\
\toprule
               & Types & Tokens \\
\midrule
Independent    &  876  & 4390   \\
ê-Conjunct     & 1480  & 6378   \\
\midrule
Total &  2356  & 10,768   \\
\bottomrule
\end{tabular}
\caption{\codebox{AWIvE} statistics \label{tab:corpstats2}
}
\end{table}


\begin{table}
\centering
\begin{tabular}{lrr} \\
\toprule
               & Types & Tokens \\
\midrule
ê-Conjunct    &  1480  & 6378   \\
kâ-Conjunct     & 600  & 1696   \\
Other-Conjunct     & 393  & 828   \\
\midrule
Total &  2473  & 8902   \\
\bottomrule
\end{tabular}
\caption{\codebox{AWCnj} statistics \label{tab:corpstats3}
}
\end{table}


\section{Univariate Analyses}
The term \textit{univariate analysis} refers to an analysis that takes into account only one variable at a time.  The most common form of univariate analysis for discrete variables is the chi-square test, originally introduced in \citet{pearson1900x} and refined over the last century to produce the modern day chi-squared test \citep{agresti2013categorical}. The chi-square test makes use of contingency tables to measure the association/correlation of a (set of) variable(s) an outcome. This is calculated by comparing the expected frequency of an outcome/variable pair with with the observed frequencies of the same pairings. Chi-square tests provide a simple statistic, the eponymous $\chi$\textsuperscript{2} statistic, whose value reflects an estimated association. This statistic is given for the whole \textit{set} of values of the explanatory and outcome variables tested. If one were to run a chi-square test to determine if the set of variables \{\codebox{1Sg.actor}, \codebox{2sg.actor}, \codebox{3sg.actor}, \codebox{past tense}, \codebox{future tense}, \codebox{present tense}\} was associated with an increased likelihood of a lemma being in the Independent or Conjunct Order, the resulting $\chi$\textsuperscript{2} statistic would indicate the level of association for that set as a whole. To investigate the effect an individual variable has, one must make use of the Standardized Pearson Residual, calculated through the formula in (\ref{presid}), where \textit{P} is the Standardized Pearson Residual, \textit{O} is the observed frequency of a variable/outcome pair, \textit{E} is the Expected frequency of a variable/outcome pair, $t_{i}$ is the sum of a variable across all outcomes, and $t_{j}$ is the sum of all variables for a given outcome  (adapted from \citep[81]{agresti2013categorical}). Note that in (\ref{presid}) the denominator represents its standard error.

\begin{equation}
P = \frac{O - E}{\sqrt{E(1-t_{i})(1-t_{j})}}
\label{presid}
\end{equation}

This produces a Standardized Residual which can be interpreted based on its magnitude and direction. A positive residual of at least 2.00 represents a significant positive association (i.e. one observes more instances of a variable/outcome pairing than would be expected) while a negative value of -2.00 or lower represents a negative association. Values greater than -2.00 but less than 2.00 represent an association not deemed to be significant (Agresti 2013, 81; exemplified in Arppe 2008, 79). 

The chi-square test is best used with higher frequency data sets. According to \citet{cochran1954some}, the results of a chi-square test are not reliable when the contingency tables for a given variable has more than 20\% of its expected values $<$5. In these cases, it is suggested that researchers make use of an alternative test, such as the Fisher's Exact Test that forms the basis of Gries' Collostructional Analysis (\citeyear{gries2004extending}). Some authors, however, believe that Fisher's Exact Test is too conservative \citep{dagostino}, increasing the risk for Type II errors in hypothesis testing. For this dissertation, I will simply consider phenomena with sufficient frequencies for a chi-square statistic.



\subsubsection{Univariate Models}
In building models for univariate analysis, all variables with a minimum occurrence of 10 were selected for a given conjugation class for each alternation. This restriction was chosen to exclude incredibly infrequent items which make statistical modelling difficult or unreliable, while including as many variables as possible. Because univariate analysis considers variables on their own basis, manual scrutiny of variable selection was not performed at this point. 

\section{Bivariate Analyses}
Following \citet{arppe2008univariate}, after univariate analyses were conducted and a set of variables were selected, I conducted bivariate analyses. Bivariate analysis is simply measuring the assosciation between two variables. Bivariate analysis as done by \citet{arppe2008univariate} can be a useful tool for creating models for mixed effects modelling. Bivariate analysis for this dissertation makes use of the \codebox{associations} function from the \codebox{polytmous} package \citep{polytomous}. This function calculates Theil's Uncertainty Coefficient \citep{theil} for every combination of variables passed to it. This coefficient is a mutual information measure and describes the extent to which knowing about one variable can inform our understanding of another variable via a reduction of entropy \citep[90]{arppe2008univariate}. 


\subsection{Bivariate models}

For this dissertation, I make use of Theil's Uncertainty Coefficient to identify to identify potential covariance which could impede the fitting of mixed effects models. Bivariance was tested for each of the four alternations mentioned above. Variables for each alternation were chosen only from those items with a significant $\chi$\textsuperscript{2} statistic (p $<$ 0.05). Automatic and manual classes were tested separately, as there was a great deal bivariance between automatic and manual class variables. 

\section{Multivariate Analysis}

Using the methodology of \citet{arppe2008univariate}, following bivariate analysis, the resulting variable sets were used to form a set of variables to perform multivariate analysis. The fundamental technique used in this analysis was logistic mixed effects regression. Logistic regression is a generalized form of linear regression as applied to categorical outcomes. Logistic regression models a binary outcome such to what extent individual predictor affects an outcome  \citep[163]{agresti2013categorical}. Like all generalized linear models, logistic regression attempts to predict outcomes by representing the distribution of the data. Specifically, the technique allows researchers to specify a set of predictors and models the data so that  researchers can determine the extent to which an individual predictor influences a particular outcome given a set of parameters (variables/effects).

Logistic regression (with a single independent variable, for example) can be modelled with the equation in (\ref{logistic}) \citep[163]{agresti2013categorical}, where $x$ represents the independent variable, $\alpha$ represents a model intercept, and $\beta$ is the slope of $x$. 

\begin{equation}
\pi(x) = \frac{e^{\alpha + \beta x}}{1+e^{\alpha + \beta x}}
\label{logistic}
\end{equation}


Equation (\ref{logistic}) represents the odds ratio for the effect of an independent variable on a particular outcome (e.g. the effect of age on the use of one of two synonyms). These ratios are bounded between 0 and $\infty$. More commonly, logistic regression models are fit with the logit function, derived from (\ref{logistic}) and seen in (\ref{logit}) \citep[163]{agresti2013categorical}. 

\begin{equation}
logit[\pi(x)] = \log \frac{\pi(x)}{1-\pi(x)} = \alpha + \beta x
\label{logit}
\end{equation}

The resulting estimates given by the logit are given in log odds, rather than odds. These values are \textit{not} bounded between 0 and $\infty$, but instead $-\infty$ and $+\infty$. Positive values represent an increase in likelihood of a an outcome for a particular variable; negative values represent a decrease in likelihood; a value of zero represents no effect on the outcome. 


This dissertation makes use of mixed effects models in its logistic regression. In terms of regression for language data, mixed effects models have now become the norm \citep[100]{barth2018evaluating}. In comparison to models that make use of only fixed effects, those variables for which all possible values are represented in the data, mixed effects models allow for the researcher to control for variables in which random variation can be expected \citep{baayen2012mixed}. For the data used in this dissertation, morphosyntactic features like \codebox{Actor.1}, which are dummy variables that represent the presence or absence of a feature (in this case, whether or not a verb is marked for first person), are \textbf{fixed effects} because all possible values (\codebox{TRUE, FALSE}) are represented in the data. Conversely, the \codebox{Lemma} variable (a multi-level variable containing all lemmas of the corpus) are \textit{samples} of the total lemma set in Nêhiyawêwein and thus can be expected to contain some amount of random variability/outcomes not present in the corpus; thus, \codebox{Lemma} is best modelled as a \textbf{random effect}. In a mixed effects model, the random variability of a random effect is `controlled' for, allowing for estimations of fixed effects without the confounds of the random effect. 

Fixed effects are analyzed relatively straightforwardly: for each of the dummy variables, one of the two possible levels are chosen to act as a baseline reference \citep{baayen2012mixed}. By default, R uses the \codebox{0/FALSE} level as a base line, though one could use the alternate level as a reference if needed. For the dummy variables in this dissertation, this means the reference level represents the absence of a particular variable. In modelling an outcome, the logistic regression analyses each observation in its training data and, if an outcome is \textit{not} observed, assigns the variable a value of 0 for the outcome; otherwise, if the variable \textit{is} observed, a value of 1 is given to the variable for the outcome \cite{baayen2012mixed}. Importantly, a model's intercept represents the variables' reference levels \citep{baayen2012mixed}. Random effects are not given a reference level; instead, each level can be thought of as adjustments to each fixed effect \citep{baayen2012mixed}. As an example, given the fixed effect \codebox{actor.1}, the logistic model would make adjustments to \codebox{actor.1}'s slope based on observations of each level of \codebox{Lemma}. In this sense, there is no reference level the others are compared to. 

This analysis makes use of the \codebox{lme4} package in R \citep{lme4}.


\subsection{Binarization of the Alternation}
Making use of logistic regression, this dissertation will investigate the behaviour of four three alternations based on the order presented in Chapter \ref{ch:order}:
\begin{enumerate}
    \item The Independent vs. the Conjunct (as a whole)
    \item The Independent vs. the ê-Conjunct
    \item The ê-Conjucnt vs. the kâ-Conjunct vs. the Other-Conjunct (Comparing modes within the Conjuncts)

\end{enumerate}

The comparisons made here allow for investigation of a a wide range of Order behaviour, while still specifying the alternations not generally explainable by semantics: The ê-Conjunct and the Independent along with the general Conjunct and the Independent. 

Logistic regression assumes a dichotomous decision by default. This is the case, for example, when comparing the Independent and the Conjunct. For the final two alternations above, however, there are more than two outcomes being compared. In polytomous cases, there are multiple methods by which the data can be binarized. One such technique is the \textit{one-vs-rest} (OVR) heuristic. In one-vs-rest comparisons, a model compares one class against all other possible classes \citep{Kramer2004}. In this way it can be thought of as comparing $x$ against $\neg x$. One can also make use of the \textit{pairwise comparison}, which creates pairs of the outcome given a predictor, tests each pair, and selects which outcome is most likely to occur with each predictor \citep{Kramer2004}.In the present study, each Conjunct class would be paired with one other class at a time, eventually being paired with each other Conjunct type. For each pairing, which-ever type is most probable given each set of predictors, would be picked as the 'winner' for that pairing, and would be proposed to the model. For each morphological feature, that class which is most often proposed would be selected and given as the likeliest Order type. In addition to one-vs-rest and pairwise comparisons, we can make use of \textit{nested dichotomies}. The concept of a nested dichotomy is straightforwardly described: a group of outcomes being compared is split into mutually exclusive dichotomies repeatedly until unary classifications are created \citep{Kramer2004}. Each dichotomy is given a statistical probability which can be multiplied together to determine the overall probability of the dichotomous tree. Similarly, other binarization techniques such as Baseline-Category \citep[468]{fox1997applied} classification offer alternative binarization techniques. \citet{arppe2008univariate} presents a brief overview of all the above techniques. For this dissertation, I will use the OVR heuristic in the polytomous Conjunct Type alternation due to its conceptual and computational ease and simplicity, particularly in modelling maximally three outcomes in a single alternation.


The resulting logistic models provide estimated adjustments for every variable, even if these effects are not considered statistically significant. The results provided by the \codebox{lme4} package calculates the p-value for each effect using the asymptotic Wald tests for generalized linear models \citep{lme4}. Recognizing that the use of p-values are not without controversy \citep{gelman2016problems}, this dissertation will still use p-values to determine which effects are most pertinent in modelling the Order alternation.

\section{Model Assessment}

In addition to the results described above, one can assess the overall performance of a logistic model. This assessment gives us invaluable information and allows us to see how well, and in what ways, a model represents Order type selection in terms of morphosyntactic and semantic features. In particular, one can see how well a model is able to evaluate a given form as the correct Order type without raising false positives (precision), as well as how many instances of a given Order type it classifies correctly, regardless of false positives (recall). Recall, precision, and overall accuracy are measured per Conjunct type for each conjugation class (not for the model in its entirety). As an additional way of assessing model fit, one may also use the $\tau$ value to determine how much better the model performs than selecting based solely on over-all proportions/through random assignment \citep[745-747]{goodman1959measures} (ranging from 0.00 to 1.00, with 1.00 being a perfect model). According to \citet[140]{arppe2008univariate}, $\tau$ values of roughly 0.5 and above suggest a `good' model fit. Because the models used in this dissertation are logistic, a true \textit{R^{2} Likelihood} score is inapplicable. Instead, a so called Pseudo-\textit{R^{2}} value must be used. As \citet[167]{Hosmer2000} point out that Pseudo-\textit{R^{2}} Likelihood scores for logistic regression are generally much smaller than in other statistics, such as \textit{R^{2}} values given in standard linear models. Another important difference between the \textit{R^{2}} measure and Pseudo-\textit{R^{2}} Likelihood is that the former can be used as a measure of how much variance is explained by the model under consideration; Pseudo-\textit{R^{2}} Likelihood can never report explained variance \citep[164]{Hosmer2000}. Instead, Pseudo-\textit{R^{2}} Likelihood can be seen as a measure of reduction in the \textit{badness} of fit. The specific form of Pseudo-\textit{R^{2}} that I will use is McFadden's Pseudo-\textit{R^{2}} ($\rho^{2}$) \citep{domencich1975urban} as reported by the \codebox{ModelStatistics} function \citep{polytomous}. Macfadden's Pseudo-\textit{R^{2}} appears to have a stable, but non-linear, relationship with a general \textit{R^{2}}, wherein a $\rho^{2}$ value of 0.2, 0.3, and 0.4 are roughly equivalent to an \textit{R^{2}} of 0.3, 0.5, and 0.73 respectively \citep[124]{domencich1975urban}. As with other Pseudo-\textit{R^{2}} measures, a $\rho^{2}$ of over 0.25 is indicative of a fairly well fit model. Furether, \citet{wan2013} suggest that, in their experience, Pseudo-\textit{R^{2}} Likelihood scores of nearly 0.30 are indicative of very good models without risk of over-fitting.

Because models are fit separately for each Conjunct type in each conjugation class in the Conjunct Type alternation, estimated probabilities add up to something close to, but not exactly, 1.00. In order to achieve this range of 0.00-1.00, the \codebox{ModelStatistics} function \citep{polytomous} aggregates all models and performs a normalization of estimated probabilities, such that they add up to exactly one.

In addition to the models described above, one can use logistic regression with only random effects. These models present only those lemma-specific effect, and can be used as a base line against which one can assess how much of an effect morphological features have on the ability to predict Conjunct type (cf. the discussion of \citet{HarriganArppe2015} regarding lemma-specific preferences on occurrence in the Independent or Conjunct orders). For each of the mixed-effect models, Pseudo-\textit{R^{2}} Likelihood, Accuracy, and $\tau$ measures for models with only random effects will be also be given. By comparing fixed effects models against the mixed effects models, we can determine the extent to which random effects affect the fit of our modelling of Order.

Based on the above methodology, the following predictions are proposed:

\begin{enumerate}
    \item Overall, modelling will be successful though constrained (likely due to the small size of the corpus).
    \item Due to a lack of syntactic data, mixed effects modelling based on the Nêhiyawêwin corpus will be able to provide some insights, but model fits will rarely be significantly informative (as measured by the exceeding of an $\rho^{2}$ of 0.2).
    \item Semantic variables will do more to explain variance than morphological variables (as in \citep{arppe2008univariate}).
    \item The Conjunct Type alternation will be significantly less cohesive in its results (due to the straightforward syntactic/semantic choices driving it, which are not reflected in the variables of the data set).
    \item The alternation between the Independent and the ê-Conjunct will be the most robust and well-fit model (because the two forms are nearly synonymous in many cases).
\end{enumerate}